{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = Adam()#SGD();\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESHAPED = 784 #X_rain은 60000개의 행으로 구성되고 28*28개의 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, RESHAPED) #28*28이미지를 784로 변환\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32') #이미지의 dtype변환\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test #class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train_samples\n",
      "10000 train_samples\n"
     ]
    }
   ],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train_samples')\n",
    "print(X_test.shape[0], 'train_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#범주 벡터를 이진 범주 벡터 행렬로 변환\n",
    "Y_train = np_utils.to_categorical(Y_train,NB_CLASSES) # 7 = [0,0,0,0,0,0,1,0,0,0]\n",
    "Y_test = np_utils.to_categorical(Y_test,NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dropout(DROPOUT))\n",
    "nn.add(Dense(N_HIDDEN))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dropout(DROPOUT))\n",
    "nn.add(Dense(NB_CLASSES))\n",
    "nn.add(Activation('softmax'))\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.5156 - acc: 0.8422 - val_loss: 0.1929 - val_acc: 0.9431\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2371 - acc: 0.9292 - val_loss: 0.1520 - val_acc: 0.9556\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1842 - acc: 0.9452 - val_loss: 0.1193 - val_acc: 0.9648\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1530 - acc: 0.9541 - val_loss: 0.1042 - val_acc: 0.9687\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1338 - acc: 0.9591 - val_loss: 0.0980 - val_acc: 0.9727\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1160 - acc: 0.9639 - val_loss: 0.0935 - val_acc: 0.9723\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1059 - acc: 0.9683 - val_loss: 0.0865 - val_acc: 0.9745\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0957 - acc: 0.9703 - val_loss: 0.0854 - val_acc: 0.9757\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0906 - acc: 0.9716 - val_loss: 0.0853 - val_acc: 0.9747\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0841 - acc: 0.9735 - val_loss: 0.0805 - val_acc: 0.9765\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0782 - acc: 0.9753 - val_loss: 0.0876 - val_acc: 0.9756\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0726 - acc: 0.9762 - val_loss: 0.0825 - val_acc: 0.9767\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0706 - acc: 0.9775 - val_loss: 0.0803 - val_acc: 0.9778\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0680 - acc: 0.9777 - val_loss: 0.0751 - val_acc: 0.9787\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0633 - acc: 0.9798 - val_loss: 0.0788 - val_acc: 0.9790\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0599 - acc: 0.9799 - val_loss: 0.0815 - val_acc: 0.9775\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0604 - acc: 0.9801 - val_loss: 0.0794 - val_acc: 0.9795\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0598 - acc: 0.9804 - val_loss: 0.0813 - val_acc: 0.9784\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0554 - acc: 0.9819 - val_loss: 0.0839 - val_acc: 0.9800\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0517 - acc: 0.9834 - val_loss: 0.0878 - val_acc: 0.9769\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0512 - acc: 0.9829 - val_loss: 0.0794 - val_acc: 0.9790\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0512 - acc: 0.9834 - val_loss: 0.0836 - val_acc: 0.9780\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0485 - acc: 0.9835 - val_loss: 0.0810 - val_acc: 0.9793\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0474 - acc: 0.9844 - val_loss: 0.0832 - val_acc: 0.9782\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0462 - acc: 0.9843 - val_loss: 0.0822 - val_acc: 0.9803\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0430 - acc: 0.9854 - val_loss: 0.0839 - val_acc: 0.9799\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0431 - acc: 0.9859 - val_loss: 0.0832 - val_acc: 0.9802\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0437 - acc: 0.9851 - val_loss: 0.0772 - val_acc: 0.9809\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0435 - acc: 0.9859 - val_loss: 0.0826 - val_acc: 0.9794\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0415 - acc: 0.9862 - val_loss: 0.0815 - val_acc: 0.9796\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0420 - acc: 0.9857 - val_loss: 0.0790 - val_acc: 0.9808\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0406 - acc: 0.9861 - val_loss: 0.0890 - val_acc: 0.9776\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0377 - acc: 0.9867 - val_loss: 0.0883 - val_acc: 0.9785\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0397 - acc: 0.9871 - val_loss: 0.0821 - val_acc: 0.9795\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0377 - acc: 0.9875 - val_loss: 0.0847 - val_acc: 0.9807\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0381 - acc: 0.9875 - val_loss: 0.0875 - val_acc: 0.9798\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0399 - acc: 0.9866 - val_loss: 0.0791 - val_acc: 0.9807\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0378 - acc: 0.9875 - val_loss: 0.0828 - val_acc: 0.9817\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0340 - acc: 0.9887 - val_loss: 0.0877 - val_acc: 0.9791\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0356 - acc: 0.9882 - val_loss: 0.0880 - val_acc: 0.9792\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0332 - acc: 0.9890 - val_loss: 0.0843 - val_acc: 0.9799\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0333 - acc: 0.9887 - val_loss: 0.0832 - val_acc: 0.9802\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0343 - acc: 0.9887 - val_loss: 0.0821 - val_acc: 0.9804\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0351 - acc: 0.9883 - val_loss: 0.0834 - val_acc: 0.9804\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0333 - acc: 0.9891 - val_loss: 0.0852 - val_acc: 0.9804\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0321 - acc: 0.9896 - val_loss: 0.0857 - val_acc: 0.9799\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0340 - acc: 0.9887 - val_loss: 0.0882 - val_acc: 0.9789\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0320 - acc: 0.9893 - val_loss: 0.0858 - val_acc: 0.9811\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0327 - acc: 0.9889 - val_loss: 0.0942 - val_acc: 0.9790\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0329 - acc: 0.9891 - val_loss: 0.0851 - val_acc: 0.9798\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0290 - acc: 0.9902 - val_loss: 0.0868 - val_acc: 0.9805\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0298 - acc: 0.9905 - val_loss: 0.0868 - val_acc: 0.9799\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0325 - acc: 0.9892 - val_loss: 0.0897 - val_acc: 0.9796\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0323 - acc: 0.9896 - val_loss: 0.0856 - val_acc: 0.9807\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0289 - acc: 0.9908 - val_loss: 0.0919 - val_acc: 0.9801\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0296 - acc: 0.9905 - val_loss: 0.0919 - val_acc: 0.9797\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0313 - acc: 0.9898 - val_loss: 0.0916 - val_acc: 0.9795\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0316 - acc: 0.9893 - val_loss: 0.0922 - val_acc: 0.9801\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0264 - acc: 0.9912 - val_loss: 0.0885 - val_acc: 0.9805\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0288 - acc: 0.9911 - val_loss: 0.0878 - val_acc: 0.9804\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0265 - acc: 0.9914 - val_loss: 0.0911 - val_acc: 0.9806\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0282 - acc: 0.9908 - val_loss: 0.0921 - val_acc: 0.9800\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0260 - acc: 0.9912 - val_loss: 0.0949 - val_acc: 0.9807\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 3s - loss: 0.0293 - acc: 0.9905 - val_loss: 0.0951 - val_acc: 0.9805\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0287 - acc: 0.9906 - val_loss: 0.0842 - val_acc: 0.9822\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0246 - acc: 0.9917 - val_loss: 0.0980 - val_acc: 0.9802\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0274 - acc: 0.9909 - val_loss: 0.0872 - val_acc: 0.9810\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0284 - acc: 0.9906 - val_loss: 0.0930 - val_acc: 0.9815\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0269 - acc: 0.9911 - val_loss: 0.0925 - val_acc: 0.9818\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0261 - acc: 0.9918 - val_loss: 0.1003 - val_acc: 0.9807\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0272 - acc: 0.9908 - val_loss: 0.0943 - val_acc: 0.9807\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0259 - acc: 0.9914 - val_loss: 0.0890 - val_acc: 0.9810\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0247 - acc: 0.9917 - val_loss: 0.0959 - val_acc: 0.9807\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0255 - acc: 0.9915 - val_loss: 0.0942 - val_acc: 0.9809\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0266 - acc: 0.9912 - val_loss: 0.0909 - val_acc: 0.9812\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0248 - acc: 0.9917 - val_loss: 0.0918 - val_acc: 0.9818\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0254 - acc: 0.9912 - val_loss: 0.0978 - val_acc: 0.9810\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0968 - val_acc: 0.9800\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0239 - acc: 0.9919 - val_loss: 0.1000 - val_acc: 0.9790\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0227 - acc: 0.9926 - val_loss: 0.1092 - val_acc: 0.9796\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0251 - acc: 0.9917 - val_loss: 0.0993 - val_acc: 0.9806\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0253 - acc: 0.9918 - val_loss: 0.0954 - val_acc: 0.9808\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0265 - acc: 0.9915 - val_loss: 0.0960 - val_acc: 0.9811\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0252 - acc: 0.9918 - val_loss: 0.0944 - val_acc: 0.9796\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0243 - acc: 0.9922 - val_loss: 0.1003 - val_acc: 0.9800\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0233 - acc: 0.9927 - val_loss: 0.0972 - val_acc: 0.9804\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0216 - acc: 0.9931 - val_loss: 0.0978 - val_acc: 0.9800\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0219 - acc: 0.9930 - val_loss: 0.0977 - val_acc: 0.9804\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0223 - acc: 0.9930 - val_loss: 0.0911 - val_acc: 0.9807\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0231 - acc: 0.9924 - val_loss: 0.1009 - val_acc: 0.9799\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0264 - acc: 0.9914 - val_loss: 0.0952 - val_acc: 0.9813\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0228 - acc: 0.9924 - val_loss: 0.0950 - val_acc: 0.9806\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0234 - acc: 0.9923 - val_loss: 0.0939 - val_acc: 0.9813\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0201 - acc: 0.9931 - val_loss: 0.0935 - val_acc: 0.9803\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0221 - acc: 0.9928 - val_loss: 0.0977 - val_acc: 0.9804\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0232 - acc: 0.9922 - val_loss: 0.0938 - val_acc: 0.9817\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0226 - acc: 0.9931 - val_loss: 0.0940 - val_acc: 0.9807\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0243 - acc: 0.9918 - val_loss: 0.0965 - val_acc: 0.9803\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0227 - acc: 0.9928 - val_loss: 0.0936 - val_acc: 0.9799\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0207 - acc: 0.9930 - val_loss: 0.0934 - val_acc: 0.9826\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0219 - acc: 0.9929 - val_loss: 0.1001 - val_acc: 0.9797\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0933 - val_acc: 0.9812\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0227 - acc: 0.9928 - val_loss: 0.0926 - val_acc: 0.9810\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0212 - acc: 0.9932 - val_loss: 0.1002 - val_acc: 0.9802\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0196 - acc: 0.9933 - val_loss: 0.1016 - val_acc: 0.9804\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0204 - acc: 0.9931 - val_loss: 0.0962 - val_acc: 0.9801\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0207 - acc: 0.9937 - val_loss: 0.0927 - val_acc: 0.9807\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0202 - acc: 0.9935 - val_loss: 0.0939 - val_acc: 0.9814\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0218 - acc: 0.9928 - val_loss: 0.0958 - val_acc: 0.9820\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0204 - acc: 0.9934 - val_loss: 0.0947 - val_acc: 0.9809\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0207 - acc: 0.9932 - val_loss: 0.0992 - val_acc: 0.9796\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0203 - acc: 0.9935 - val_loss: 0.1007 - val_acc: 0.9810\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0204 - acc: 0.9935 - val_loss: 0.1027 - val_acc: 0.9811\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0217 - acc: 0.9929 - val_loss: 0.0973 - val_acc: 0.9807\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0210 - acc: 0.9932 - val_loss: 0.0965 - val_acc: 0.9807\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0203 - acc: 0.9936 - val_loss: 0.1023 - val_acc: 0.9801\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0211 - acc: 0.9930 - val_loss: 0.0975 - val_acc: 0.9807\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0966 - val_acc: 0.9823\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.1018 - val_acc: 0.9801\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0208 - acc: 0.9933 - val_loss: 0.1048 - val_acc: 0.9802\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0234 - acc: 0.9928 - val_loss: 0.0983 - val_acc: 0.9811\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0196 - acc: 0.9935 - val_loss: 0.1125 - val_acc: 0.9798\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0197 - acc: 0.9932 - val_loss: 0.0963 - val_acc: 0.9819\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0222 - acc: 0.9929 - val_loss: 0.1016 - val_acc: 0.9798\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0200 - acc: 0.9934 - val_loss: 0.0984 - val_acc: 0.9814\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0228 - acc: 0.9926 - val_loss: 0.0994 - val_acc: 0.9800\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 3s - loss: 0.0184 - acc: 0.9940 - val_loss: 0.1020 - val_acc: 0.9818\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0197 - acc: 0.9939 - val_loss: 0.1034 - val_acc: 0.9802\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0202 - acc: 0.9934 - val_loss: 0.1046 - val_acc: 0.9798\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0204 - acc: 0.9935 - val_loss: 0.1058 - val_acc: 0.9804\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0213 - acc: 0.9936 - val_loss: 0.1024 - val_acc: 0.9800\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0204 - acc: 0.9931 - val_loss: 0.1007 - val_acc: 0.9817\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0198 - acc: 0.9935 - val_loss: 0.1036 - val_acc: 0.9808\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0197 - acc: 0.9935 - val_loss: 0.1046 - val_acc: 0.9798\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0188 - acc: 0.9938 - val_loss: 0.1037 - val_acc: 0.9808\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0184 - acc: 0.9942 - val_loss: 0.1053 - val_acc: 0.9803\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0195 - acc: 0.9937 - val_loss: 0.1067 - val_acc: 0.9803\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0223 - acc: 0.9930 - val_loss: 0.1006 - val_acc: 0.9803\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0192 - acc: 0.9936 - val_loss: 0.1052 - val_acc: 0.9803\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0196 - acc: 0.9938 - val_loss: 0.1077 - val_acc: 0.9802\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0202 - acc: 0.9938 - val_loss: 0.0986 - val_acc: 0.9805\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0176 - acc: 0.9941 - val_loss: 0.1036 - val_acc: 0.9808\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0190 - acc: 0.9939 - val_loss: 0.1046 - val_acc: 0.9802\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0179 - acc: 0.9945 - val_loss: 0.1084 - val_acc: 0.9800\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0183 - acc: 0.9940 - val_loss: 0.1055 - val_acc: 0.9798\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0191 - acc: 0.9936 - val_loss: 0.1089 - val_acc: 0.9798\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0198 - acc: 0.9934 - val_loss: 0.1122 - val_acc: 0.9800\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0200 - acc: 0.9937 - val_loss: 0.1092 - val_acc: 0.9795\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0185 - acc: 0.9943 - val_loss: 0.1086 - val_acc: 0.9807\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0183 - acc: 0.9941 - val_loss: 0.1089 - val_acc: 0.9804\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0190 - acc: 0.9938 - val_loss: 0.1055 - val_acc: 0.9816\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0164 - acc: 0.9946 - val_loss: 0.1082 - val_acc: 0.9811\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0186 - acc: 0.9938 - val_loss: 0.1042 - val_acc: 0.9818\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0191 - acc: 0.9935 - val_loss: 0.1038 - val_acc: 0.9809\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0175 - acc: 0.9940 - val_loss: 0.1061 - val_acc: 0.9804\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0161 - acc: 0.9946 - val_loss: 0.1127 - val_acc: 0.9805\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0168 - acc: 0.9946 - val_loss: 0.1137 - val_acc: 0.9798\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0212 - acc: 0.9932 - val_loss: 0.1054 - val_acc: 0.9811\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0182 - acc: 0.9943 - val_loss: 0.1047 - val_acc: 0.9820\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0188 - acc: 0.9942 - val_loss: 0.1055 - val_acc: 0.9799\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0174 - acc: 0.9947 - val_loss: 0.1119 - val_acc: 0.9798\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0160 - acc: 0.9951 - val_loss: 0.1097 - val_acc: 0.9808\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0178 - acc: 0.9943 - val_loss: 0.1075 - val_acc: 0.9800\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0175 - acc: 0.9944 - val_loss: 0.1056 - val_acc: 0.9816\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0188 - acc: 0.9940 - val_loss: 0.1088 - val_acc: 0.9811\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0170 - acc: 0.9944 - val_loss: 0.1143 - val_acc: 0.9797\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0172 - acc: 0.9945 - val_loss: 0.1117 - val_acc: 0.9813\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0181 - acc: 0.9937 - val_loss: 0.1105 - val_acc: 0.9803\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0166 - acc: 0.9948 - val_loss: 0.1087 - val_acc: 0.9825\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0164 - acc: 0.9950 - val_loss: 0.1153 - val_acc: 0.9813\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0197 - acc: 0.9937 - val_loss: 0.1120 - val_acc: 0.9802\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0166 - acc: 0.9943 - val_loss: 0.1114 - val_acc: 0.9812\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0161 - acc: 0.9948 - val_loss: 0.1050 - val_acc: 0.9816\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0171 - acc: 0.9946 - val_loss: 0.1073 - val_acc: 0.9817\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0172 - acc: 0.9947 - val_loss: 0.1179 - val_acc: 0.9803\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0185 - acc: 0.9945 - val_loss: 0.1185 - val_acc: 0.9802\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0176 - acc: 0.9946 - val_loss: 0.1079 - val_acc: 0.9804\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0203 - acc: 0.9934 - val_loss: 0.1100 - val_acc: 0.9808\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0168 - acc: 0.9947 - val_loss: 0.1054 - val_acc: 0.9812\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0150 - acc: 0.9952 - val_loss: 0.1166 - val_acc: 0.9789\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0186 - acc: 0.9942 - val_loss: 0.1075 - val_acc: 0.9803\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0178 - acc: 0.9949 - val_loss: 0.1142 - val_acc: 0.9798\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0159 - acc: 0.9947 - val_loss: 0.1116 - val_acc: 0.9799\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0189 - acc: 0.9943 - val_loss: 0.1165 - val_acc: 0.9791\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0168 - acc: 0.9946 - val_loss: 0.1201 - val_acc: 0.9800\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0162 - acc: 0.9947 - val_loss: 0.1207 - val_acc: 0.9790\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0159 - acc: 0.9947 - val_loss: 0.1162 - val_acc: 0.9798\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0168 - acc: 0.9950 - val_loss: 0.1121 - val_acc: 0.9800\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0181 - acc: 0.9942 - val_loss: 0.1132 - val_acc: 0.9807\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 2s - loss: 0.0175 - acc: 0.9948 - val_loss: 0.1115 - val_acc: 0.9802\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0161 - acc: 0.9945 - val_loss: 0.1154 - val_acc: 0.9807\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0164 - acc: 0.9949 - val_loss: 0.1212 - val_acc: 0.9773\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0175 - acc: 0.9945 - val_loss: 0.1127 - val_acc: 0.9800\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0168 - acc: 0.9948 - val_loss: 0.1089 - val_acc: 0.9803\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0196 - acc: 0.9943 - val_loss: 0.1116 - val_acc: 0.9808\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.0172 - acc: 0.9946 - val_loss: 0.1104 - val_acc: 0.9806\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0171 - acc: 0.9945 - val_loss: 0.1142 - val_acc: 0.9793\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0157 - acc: 0.9948 - val_loss: 0.1117 - val_acc: 0.9798\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0160 - acc: 0.9951 - val_loss: 0.1142 - val_acc: 0.9809\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0164 - acc: 0.9949 - val_loss: 0.1140 - val_acc: 0.9801\n"
     ]
    }
   ],
   "source": [
    "history = nn.fit(X_train,Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (60000, 784))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9536/10000 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "score = nn.evaluate(X_test, Y_test, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test score :  0.124331078761\n",
      "Test accuracy :  0.9786\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest score : \",score[0])\n",
    "print(\"Test accuracy : \",score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb32bf6b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accurancy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "fig = plt.gcf() #show 전에 해야한다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig.savefig('./save/plt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
